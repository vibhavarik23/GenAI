{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e265d19a",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ebbc27",
   "metadata": {},
   "source": [
    "#### Sample Dataset used \n",
    "https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4d128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f097fe1",
   "metadata": {},
   "source": [
    "### 1. Lower Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2e49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"]=df[\"review\"].str.lower()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506850f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"review\"][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febe839",
   "metadata": {},
   "source": [
    "#### 2. Remove HTML Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b46f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tag(text):\n",
    "    pattern=re.compile(r\"<.*?>\")\n",
    "    return re.sub(pattern,r\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b013ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(remove_html_tag)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e09516",
   "metadata": {},
   "source": [
    "#### 3. Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b230d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url(text):\n",
    "    pattern=re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return re.sub(pattern,r\"\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74179dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ef386",
   "metadata": {},
   "source": [
    "#### 4. Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae381fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "exclude = string.punctuation\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a897a633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1: Time consuming\n",
    "def remove_punctuation(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char,r\"\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5389a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2: Prefered\n",
    "def remove_punc(text):\n",
    "    return text.translate(str.maketrans('','',exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ba5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_punctuation)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31b5489",
   "metadata": {},
   "source": [
    "#### 5. Chat Conversion Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92556b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = {\n",
    "    'AFAIK':'As Far As I Know',\n",
    "    'AFK':'Away From Keyboard',\n",
    "    'ASAP':'As Soon As Possible',\n",
    "    \"FYI\": \"For Your Information\",\n",
    "    \"ASAP\": \"As Soon As Possible\",\n",
    "    \"BRB\": \"Be Right Back\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"OMG\": \"Oh My God\",\n",
    "    \"IMO\": \"In My Opinion\",\n",
    "    \"LOL\": \"Laugh Out Loud\",\n",
    "    \"TTYL\": \"Talk To You Later\",\n",
    "    \"GTG\": \"Got To Go\",\n",
    "    \"TTYT\": \"Talk To You Tomorrow\",\n",
    "    \"IDK\": \"I Don't Know\",\n",
    "    \"TMI\": \"Too Much Information\",\n",
    "    \"IMHO\": \"In My Humble Opinion\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "    \"AFAIK\": \"As Far As I Know\",\n",
    "    \"BTW\": \"By The Way\",\n",
    "    \"FAQ\": \"Frequently Asked Questions\",\n",
    "    \"TGIF\": \"Thank God It's Friday\",\n",
    "    \"FYA\": \"For Your Action\",\n",
    "    \"ICYMI\": \"In Case You Missed It\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb3acfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word.upper() in chat_words:\n",
    "            new_text.append(chat_words[word.upper()])\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ce7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(chat_conversion)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d131d238",
   "metadata": {},
   "source": [
    "#### 6. Spelling Correction/Incorrect text handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ce614c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada39fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_text = 'ceertain conditionas duriing seveal ggenerations aree moodified in the saame maner.'\n",
    "\n",
    "textBlb = TextBlob(incorrect_text)\n",
    "\n",
    "textBlb.correct().string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ea25d0",
   "metadata": {},
   "source": [
    "#### 7. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87466d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2d025",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29907970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    \n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x = new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012162a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03366591",
   "metadata": {},
   "source": [
    "#### 8. Remove emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d88212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1:\n",
    "import re\n",
    "\n",
    "def remove_emoji(text):\n",
    "    pattern=re.compile(\"[\"\n",
    "                        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                        u\"\\U00002702-\\U000027B0\"\n",
    "                        u\"\\U000024C2-\\U0001F251\"\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f6d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review']=df['review'].apply(remove_emoji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2:\n",
    "\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3bad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.demojize('Python is ðŸ”¥'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab7d3a",
   "metadata": {},
   "source": [
    "### 9. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6b1d3",
   "metadata": {},
   "source": [
    "#### a. Using Spilt Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf85e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization\n",
    "df['review']=df['review'].str.split()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1359db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "df['review']=df['review'].str.split('.')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39e4158",
   "metadata": {},
   "source": [
    "#### b. Using Regular Expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf9c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization\n",
    "import re\n",
    "sent3 = 'I am going to delhi!'\n",
    "tokens = re.findall(\"[\\w']+\", sent3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50ed3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sentences = re.compile('[.!?] ').split(text)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0379ca8",
   "metadata": {},
   "source": [
    "#### c. NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb52b355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac788bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenization\n",
    "sent1 = 'I am going to visit delhi!'\n",
    "word_tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7578c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence tokenization\n",
    "text = \"\"\"Lorem Ipsum is simply dummy text of the printing and typesetting industry?\n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s,\n",
    "when an unknown printer took a galley of type and scrambled it to make a type specimen book.\"\"\"\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04bf579",
   "metadata": {},
   "source": [
    "#### d. Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673e20ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc=nlp(text)\n",
    "token_list=[token.text for token in doc]\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f5f76",
   "metadata": {},
   "source": [
    "#### e. Transformer based "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0432a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.tokenize(\"Text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "tokenizer.tokenize(\"Text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef8da5",
   "metadata": {},
   "source": [
    "### 10. Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f32099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30fe5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Text\"\n",
    "stem_word(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4248cd71",
   "metadata": {},
   "source": [
    "### 11. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "text=\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\n",
    "punctuations=\"?:!.,;\"\n",
    "tokens=nltk.word_tokenize(text)\n",
    "\n",
    "for word in tokens:\n",
    "    if word in punctuations:\n",
    "        tokens.remove(word)\n",
    "        \n",
    "print(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\n",
    "for word in sentence_words:\n",
    "    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
